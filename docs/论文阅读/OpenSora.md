# OpenSora

OpenSora 是一个优质的开源视频生成模型，它的代码应该是既有 VAE 又有 Diffusion 的，并且在持续更新中。

## 数据集

OpenSora 使用了多个开源视频和图像的数据集，然后将原始视频按帧切割(使用 PySceneCut)，然后需要过滤出高质量的视频，主要是基于以下三点：

1. 美学分数（Aesthetic Score）： 用于衡量视频帧的美学偏好，使用LAION 的评分器，并取三个采样帧的平均分数。
2. 光流分数（Optical Flow Score）： 衡量视频的动态范围，用于过滤低运动的视频，使用UniMatch 模型。
3. 光学字符识别（OCR）： 识别视频中的文本，并移除包含过多文本的视频（如新闻广播和广告）。使用了由MMOCR 实现的DBNet++模型。

此外还利用GPT-4V和PLLaVA 为视频生成了高质量的字幕。

## 模型架构

### 3D Autoencoder

使用了Stability-AI的2D VAE，它将视频在空间上压缩了8x8倍。为了减少时间维度，通过每三帧提取一帧的方式进行降采样。但是这种降采样方法导致生成视频的帧率降低，从而影响了时间上的流畅性。为了解决上述限制，Open-Sora 1.2 引入了一种受OpenAI Sora启发的视频压缩网络，实现了时间维度上4倍的压缩。这一改进消除了帧提取的需要，使得视频能够以原始帧率生成，从而提高了时间流畅性。通过观察发现，即使经过2D VAE压缩后，时间上相邻的特征仍然高度相关。基于这一洞察，开发了一个简单而有效的视频压缩网络，该网络首先进行8x8的空间压缩，随后进行4x的时间压缩。在实现方面，2D VAE使用SDXL的预训练权重进行初始化，而3D VAE则采用了Magvit-v2's VAE的架构，使整个视频压缩网络的总参数量达到3.84亿。

训练过程分为三个明确的阶段：

阶段1主要侧重于重建2D VAE压缩的特征，并运用恒等损失来对齐3D VAE的特征与2D VAE的特征，从而实现更快的收敛和更好的初始图像重建质量。

阶段2移除了恒等损失，让3D VAE能够更好地优化其时间理解能力。

阶段3则转向直接重建原始视频，因为2D VAE特征的重建不足以进一步提升效果。此阶段显著地引入了混合视频长度训练（最长可达34帧），以增强模型对不同视频时长的鲁棒性，并解决了早期阶段在非标准长度视频中出现的模糊问题。

前两个阶段使用的数据集包含80%的视频数据和20%的图像数据。最终的堆叠VAE架构展现出显著优势，包括由于输入已预压缩而在推理过程中所需的内存极少，并且能够在大幅降低计算成本的同时，实现与其它开源3D VAE可媲美的性能。

